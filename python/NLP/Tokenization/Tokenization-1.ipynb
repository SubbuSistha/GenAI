{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf925767-170c-414a-9479-26575d35db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =\"\"\" \n",
    "Tokenization in Machine Learning (ML) is the process of splitting text into smaller components, \n",
    "such as words, subwords, characters, or sentences, \n",
    "which can then be used as inputs for models. \n",
    "It’s a crucial step in Natural Language Processing (NLP), \n",
    "as most ML models can’t process raw text directly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc8ac451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\git\\genai\\python\\envk\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\git\\genai\\python\\envk\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\git\\genai\\python\\envk\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\git\\genai\\python\\envk\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\git\\genai\\python\\envk\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\git\\genai\\python\\envk\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\krith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b66ac9af-94d8-4dfa-8640-ffa289709f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Tokenization in Machine Learning (ML) is the process of splitting text into smaller components, \n",
      "such as words, subwords, characters, or sentences, \n",
      "which can then be used as inputs for models. \n",
      "It’s a crucial step in Natural Language Processing (NLP), \n",
      "as most ML models can’t process raw text directly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2faba343-823f-4809-a8c1-f54e30350c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nTokenization in Machine Learning (ML) is the process of splitting text into smaller components, \\nsuch as words, subwords, characters, or sentences, \\nwhich can then be used as inputs for models.',\n",
       " 'It’s a crucial step in Natural Language Processing (NLP), \\nas most ML models can’t process raw text directly.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40a9691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'in',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " '(',\n",
       " 'ML',\n",
       " ')',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'splitting',\n",
       " 'text',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'components',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'words',\n",
       " ',',\n",
       " 'subwords',\n",
       " ',',\n",
       " 'characters',\n",
       " ',',\n",
       " 'or',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'which',\n",
       " 'can',\n",
       " 'then',\n",
       " 'be',\n",
       " 'used',\n",
       " 'as',\n",
       " 'inputs',\n",
       " 'for',\n",
       " 'models',\n",
       " '.',\n",
       " 'It',\n",
       " '’',\n",
       " 's',\n",
       " 'a',\n",
       " 'crucial',\n",
       " 'step',\n",
       " 'in',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'as',\n",
       " 'most',\n",
       " 'ML',\n",
       " 'models',\n",
       " 'can',\n",
       " '’',\n",
       " 't',\n",
       " 'process',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'directly',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c567ce2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'in',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " '(',\n",
       " 'ML',\n",
       " ')',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'splitting',\n",
       " 'text',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'components',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'words',\n",
       " ',',\n",
       " 'subwords',\n",
       " ',',\n",
       " 'characters',\n",
       " ',',\n",
       " 'or',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'which',\n",
       " 'can',\n",
       " 'then',\n",
       " 'be',\n",
       " 'used',\n",
       " 'as',\n",
       " 'inputs',\n",
       " 'for',\n",
       " 'models.',\n",
       " 'It’s',\n",
       " 'a',\n",
       " 'crucial',\n",
       " 'step',\n",
       " 'in',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " ',',\n",
       " 'as',\n",
       " 'most',\n",
       " 'ML',\n",
       " 'models',\n",
       " 'can’t',\n",
       " 'process',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'directly',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer\n",
    "\n",
    "tree_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "tree_tokenizer.tokenize(corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
